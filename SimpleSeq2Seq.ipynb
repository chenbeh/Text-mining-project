{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.train.pairs.200K.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 200000 sentence pairs\n",
      "Trimmed to 200000 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "de 208226\n",
      "en 95003\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    #pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    i=0\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('de', 'en', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['es geht nicht an dass uber ausfuhrungsbestimmungen deren inhalt zweck und ausma vorher nicht bestimmt ist zusammen mit den nationalen burokratien das gesetzgebungsrecht des europaischen parlaments ausgehebelt wird .',\n",
       " 'it is not acceptable that with the help of the national bureaucracies parliament apos s legislative prerogative should be made null and void by means of implementing provisions whose content purpose and extent are not laid down in advance .']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_output, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)))\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def variableFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if use_cuda:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "def variablesFromPair(pair):\n",
    "    input_variable = variableFromSentence(input_lang, pair[0])\n",
    "    target_variable = variableFromSentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "   55\n",
       "   56\n",
       "   57\n",
       "   58\n",
       "   59\n",
       "   60\n",
       "   61\n",
       "    7\n",
       "    1\n",
       "[torch.LongTensor of size 9x1]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variableFromSentence(input_lang, pairs[9][0])\n",
    "#indexesFromSentence(input_lang, pairs[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3114"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "    \n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "   \n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            decoder_input = target_variable[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            \n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            \n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            if ni == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [variablesFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "        \n",
    " \n",
    "        loss = train(input_variable, target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    input_variable = variableFromSentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))  # SOS\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "        decoder_attentions[di] = decoder_attention.data\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "        \n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11m 18s (- 441m 7s) (5000 2%) 5.0948\n",
      "23m 54s (- 454m 14s) (10000 5%) 3.4684\n",
      "35m 57s (- 443m 25s) (15000 7%) 1.5571\n",
      "44m 46s (- 403m 2s) (20000 10%) 0.4720\n",
      "53m 37s (- 375m 25s) (25000 12%) 0.1152\n",
      "62m 31s (- 354m 17s) (30000 15%) 0.0534\n",
      "71m 22s (- 336m 28s) (35000 17%) 0.0360\n",
      "80m 14s (- 320m 57s) (40000 20%) 0.0270\n",
      "89m 3s (- 306m 44s) (45000 22%) 0.0218\n",
      "97m 53s (- 293m 40s) (50000 25%) 0.0183\n",
      "106m 44s (- 281m 24s) (55000 27%) 0.0160\n",
      "115m 36s (- 269m 44s) (60000 30%) 0.0140\n",
      "124m 26s (- 258m 26s) (65000 32%) 0.0124\n",
      "133m 16s (- 247m 30s) (70000 35%) 0.0112\n",
      "142m 4s (- 236m 48s) (75000 37%) 0.0102\n",
      "150m 53s (- 226m 20s) (80000 40%) 0.0093\n",
      "159m 40s (- 216m 1s) (85000 42%) 0.0086\n",
      "168m 31s (- 205m 58s) (90000 45%) 0.0080\n",
      "177m 22s (- 196m 3s) (95000 47%) 0.0074\n",
      "186m 13s (- 186m 13s) (100000 50%) 0.0070\n",
      "195m 3s (- 176m 28s) (105000 52%) 0.0066\n",
      "203m 54s (- 166m 50s) (110000 55%) 0.0063\n",
      "212m 41s (- 157m 12s) (115000 57%) 0.0059\n",
      "221m 31s (- 147m 41s) (120000 60%) 0.0057\n",
      "230m 19s (- 138m 11s) (125000 62%) 0.0053\n",
      "239m 12s (- 128m 48s) (130000 65%) 0.0051\n",
      "248m 11s (- 119m 29s) (135000 67%) 0.0049\n",
      "256m 59s (- 110m 8s) (140000 70%) 0.0047\n",
      "265m 53s (- 100m 51s) (145000 72%) 0.0045\n",
      "274m 42s (- 91m 34s) (150000 75%) 0.0043\n",
      "283m 33s (- 82m 19s) (155000 77%) 0.0042\n",
      "292m 22s (- 73m 5s) (160000 80%) 0.0040\n",
      "301m 15s (- 63m 54s) (165000 82%) 0.0047\n",
      "310m 4s (- 54m 43s) (170000 85%) 0.0039\n",
      "318m 53s (- 45m 33s) (175000 87%) 0.0039\n",
      "327m 43s (- 36m 24s) (180000 90%) 0.0042\n",
      "336m 32s (- 27m 17s) (185000 92%) 0.0036\n",
      "345m 22s (- 18m 10s) (190000 95%) 0.0034\n",
      "354m 14s (- 9m 4s) (195000 97%) 0.0033\n",
      "363m 3s (- 0m 0s) (200000 100%) 0.0032\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words,\n",
    "                               1, dropout_p=0.1)\n",
    "\n",
    "if use_cuda:\n",
    "    encoder1 = encoder1.cuda()\n",
    "    attn_decoder1 = attn_decoder1.cuda()\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 200000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> click and go .\n",
      "= click and go .\n",
      "< click and go . <EOS>\n",
      "\n",
      "> und wie sieht eins mensch von innen aus .\n",
      "= und wie sieht ein mensch von innen aus .\n",
      "< und wie sieht ein mensch von innen aus . <EOS>\n",
      "\n",
      "> zudem ist den ganze schlicht unspannend .\n",
      "= zudem ist das ganze schlicht unspannend .\n",
      "< zudem ist das ganze schlicht unspannend . <EOS>\n",
      "\n",
      "> fur die landessportbund war herr delp gekommen .\n",
      "= fur den landessportbund war herr delp gekommen .\n",
      "< fur den landessportbund war herr delp gekommen . <EOS>\n",
      "\n",
      "> sehen ihr sich doch herrn messner an .\n",
      "= sehen sie sich doch herrn messner an .\n",
      "< sehen sie sich doch herrn messner an . <EOS>\n",
      "\n",
      "> es konnten aber noch mehr werden .\n",
      "= es konnten aber noch mehr werden .\n",
      "< es konnten aber noch mehr werden . <EOS>\n",
      "\n",
      "> qualitat kann man sehen .\n",
      "= qualitat kann man sehen .\n",
      "< qualitat kann man sehen . <EOS>\n",
      "\n",
      "> bedeutung den jugendalters fur interaktionspadagogische aktivitaten . . .\n",
      "= bedeutung des jugendalters fur interaktionspadagogische aktivitaten . . .\n",
      "< bedeutung des jugendalters fur interaktionspadagogische aktivitaten . . . <EOS>\n",
      "\n",
      "> der programm liegen standard scripts bei .\n",
      "= dem programm liegen standard scripts bei .\n",
      "< dem programm liegen standard scripts bei . <EOS>\n",
      "\n",
      "> des platanenallee entlang flie t schone neckar .\n",
      "= der platanenallee entlang flie t der schone neckar .\n",
      "< der platanenallee entlang flie t der schone neckar . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = so selten sind mir zu finden .\n",
      "output = so selten sind bisher zu uberlegen . <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEYCAYAAABY7FHWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niCYJDBER0SSBISIimvw39QCDMrN166MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e6d4c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "pairs\n",
    "evaluateAndShowAttention(\"so selten sind mir zu finden .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "attn_model = 'general'\n",
    "hidden_size = 256\n",
    "n_layers = 2\n",
    "dropout_p = 0.1\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, n_layers)\n",
    "decoder = AttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, n_layers, dropout_p=dropout_p)\n",
    "\n",
    "# Move models to GPU\n",
    "if use_cuda:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = 0.05\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuring training\n",
    "n_epochs = 200000\n",
    "plot_every = 5000\n",
    "print_every = 5000\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected argument self to have 1 dimension(s), but has 2 at /Users/soumith/miniconda2/conda-bld/pytorch_1503975723910/work/torch/csrc/generic/TensorMethods.cpp:23020",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-8a8c639dd94a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Run the train function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Keep track of loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-0e4ab96d8236>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Without teacher forcing: use network's own prediction as the next input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-1e8710146be2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_input, last_context, last_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B x 1 x N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-62ad5160db99>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Calculate energies for each encoder output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mattn_energies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-62ad5160db99>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, hidden, encoder_output)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'general'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menergy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menergy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_addcop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/_functions/blas.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, vector1, vector2)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvector1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvector1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected argument self to have 1 dimension(s), but has 2 at /Users/soumith/miniconda2/conda-bld/pytorch_1503975723910/work/torch/csrc/generic/TensorMethods.cpp:23020"
     ]
    }
   ],
   "source": [
    "# Begin!\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    # Get training data for this cycle\n",
    "    training_pair = variablesFromPair(random.choice(pairs))\n",
    "    input_variable = training_pair[0]\n",
    "    target_variable = training_pair[1]\n",
    "\n",
    "    # Run the train function\n",
    "    loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if epoch == 0: continue\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
